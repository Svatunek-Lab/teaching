{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Svatunek-Lab/teaching/blob/main/2025_University_of_Malaya_Intro_Chemistry_Python/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Molecules to Models – Machine Learning in Chemistry\n",
        "\n",
        "In the previous sections, we learned how to:\n",
        "- represent molecules using **SMILES** strings,\n",
        "- visualize them in **2D and 3D**, and\n",
        "- calculate **basic molecular descriptors** such as molecular weight or polar surface area.\n",
        "\n",
        "Now we will take the next step:  \n",
        "using those descriptors as **numerical features** in a simple **machine learning model**.\n",
        "\n",
        "---\n",
        "\n",
        "## What you will learn\n",
        "\n",
        "1. Load a small dataset of organic molecules with known **experimental solubility**.  \n",
        "2. Compute a few **physicochemical descriptors** using RDKit.  \n",
        "3. Train a **regression model** (a Random Forest) to predict solubility from molecular structure.  \n",
        "4. Evaluate the model and visualize how well it performs.\n",
        "\n",
        "This is a simplified example of what computational and data-driven chemistry does at larger scales —  \n",
        "transforming chemical intuition into quantitative models.\n",
        "\n",
        "---\n",
        "\n",
        "> *Goal:* Connect chemistry and data science — predicting molecular behavior from structure using only a few lines of Python.\n"
      ],
      "metadata": {
        "id": "2Hrtg50h2xKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Setup and Preparation\n",
        "\n",
        "Firstwe have to install all necessary packages —  \n",
        "**RDKit** for chemistry operations, **pandas** for data handling, **scikit-learn** for machine learning, and **matplotlib** for visualization —  \n",
        "we are ready to start working with real chemical data.\n"
      ],
      "metadata": {
        "id": "ApaDM8kH3zWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDV1Luu21SXl"
      },
      "outputs": [],
      "source": [
        "!pip -q install pandas scikit-learn matplotlib\n",
        "!pip install -q rdkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Solubility Dataset\n",
        "\n",
        "In this section, we will use a small dataset of **organic molecules** with their **experimental aqueous solubility** (log S).\n",
        "\n",
        "Each molecule is represented by its **SMILES** string (a text-based structure code), and the goal is to use this structural information to **predict solubility** using machine learning.\n",
        "\n",
        "We will:\n",
        "1. **Load** the dataset directly from this repository (stored in the `data` folder).  \n",
        "2. **Inspect** the first few entries to understand what it contains.  \n",
        "3. **Split** the data into three parts:\n",
        "   - **Training set** → used to teach the model  \n",
        "   - **Validation set** → used to tune and test during training  \n",
        "   - **Test set** → used at the end to see how well the model generalizes\n",
        "\n",
        "This is the same idea as in experimental chemistry:\n",
        "> we “train” our model with known molecules, then test whether it can correctly predict unknown ones.\n"
      ],
      "metadata": {
        "id": "vgfCubi43mrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/Svatunek-Lab/teaching/main/2025_University_of_Malaya_Intro_Chemistry_Python/data/delaney-processed.csv\"\n",
        "df = pd.read_csv(url)\n",
        "print(\"Column names:\", df.columns.tolist())\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# Keep only the essential columns\n",
        "df = df[[\"Compound ID\", \"measured log solubility in mols per litre\", \"smiles\"]]\n",
        "df.columns = [\"name\", \"logS_exp\", \"smiles\"]\n",
        "\n",
        "# Split into train, validation, and test sets 70:15:15\n",
        "train, temp = train_test_split(df, test_size=0.3, random_state=42)\n",
        "valid, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"Training set: {len(train)} molecules\")\n",
        "print(f\"Validation set: {len(valid)} molecules\")\n",
        "print(f\"Test set: {len(test)} molecules\")\n",
        "\n",
        "train.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "O_E-uN754H-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Molecular Descriptors\n",
        "\n",
        "Now that we have our training, validation, and test datasets ready,  \n",
        "the next step is to **translate the SMILES strings into numerical properties** that capture chemical information.\n",
        "\n",
        "Machine learning models cannot understand the letters and symbols in a SMILES string directly —  \n",
        "they need numbers that describe what makes one molecule different from another.\n",
        "\n",
        "These numbers are called **molecular descriptors** and include:\n",
        "\n",
        "| Descriptor | Meaning |\n",
        "|-------------|----------|\n",
        "| **Molecular Weight (MW)** | Approximate size of the molecule |\n",
        "| **logP** | Lipophilicity / hydrophobicity (how well it dissolves in fat vs. water) |\n",
        "| **TPSA** | Topological Polar Surface Area — how polar the molecule is |\n",
        "| **HBD / HBA** | Number of hydrogen bond donors and acceptors |\n",
        "| **Rotatable Bonds** | Measure of flexibility |\n",
        "\n",
        "These descriptors provide a quantitative way to describe molecules —  \n",
        "bridging chemical intuition and data science.  \n",
        "We will calculate them using **RDKit**, which can derive many such properties from a molecule’s structure.\n"
      ],
      "metadata": {
        "id": "y42vsCeU5gG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors\n",
        "import numpy as np\n",
        "\n",
        "# --- Function to compute descriptors for one molecule ---\n",
        "def compute_descriptors(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is None:\n",
        "        return None\n",
        "    mol = Chem.AddHs(mol)  # add hydrogens for surface area calculation\n",
        "\n",
        "    try:\n",
        "\n",
        "        return {\n",
        "            \"MolWt\": Descriptors.MolWt(mol),\n",
        "            \"logP\": Crippen.MolLogP(mol),\n",
        "            \"TPSA\": rdMolDescriptors.CalcTPSA(mol),\n",
        "            \"HBD\": rdMolDescriptors.CalcNumHBD(mol),\n",
        "            \"HBA\": rdMolDescriptors.CalcNumHBA(mol),\n",
        "            \"RotB\": rdMolDescriptors.CalcNumRotatableBonds(mol),\n",
        "        }\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# --- Helper function to apply on a dataframe ---\n",
        "def add_descriptors(df):\n",
        "    desc_list = []\n",
        "    for smi in df[\"smiles\"]:\n",
        "        d = compute_descriptors(smi)\n",
        "        desc_list.append(d if d else {\n",
        "            \"MolWt\": np.nan, \"logP\": np.nan, \"TPSA\": np.nan, \"ASA\": np.nan,\n",
        "            \"PolarPercent\": np.nan, \"HBD\": np.nan, \"HBA\": np.nan, \"RotB\": np.nan\n",
        "        })\n",
        "    desc_df = pd.DataFrame(desc_list)\n",
        "    return pd.concat([df.reset_index(drop=True), desc_df], axis=1)\n",
        "\n",
        "# --- Apply to each set ---\n",
        "train_desc = add_descriptors(train)\n",
        "valid_desc = add_descriptors(valid)\n",
        "test_desc  = add_descriptors(test)\n",
        "\n",
        "print(\"Train shape:\", train_desc.shape)\n",
        "train_desc.head()\n"
      ],
      "metadata": {
        "id": "hUDEMxdJ6AQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Datasets\n",
        "\n",
        "Before training a machine learning model, it is always a good idea to **look at the data**.  \n",
        "We can visualize the distribution of some molecular properties to understand:\n",
        "\n",
        "- How diverse our molecules are in terms of size, polarity, and flexibility  \n",
        "- Whether the **training**, **validation**, and **test** sets look similar  \n",
        "- Whether any dataset is biased toward small or large molecules  \n",
        "\n",
        "If the distributions are similar, we can expect the model to generalize better.  \n",
        "We will compare histograms of **molecular weight**, **logP**, and **TPSA** across the three datasets.\n"
      ],
      "metadata": {
        "id": "W4FMQKwD7rjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# choose a consistent style\n",
        "plt.style.use(\"default\")\n",
        "\n",
        "# define which descriptors to compare\n",
        "features = [\"MolWt\", \"logP\", \"TPSA\"]\n",
        "\n",
        "# plot histograms for each descriptor\n",
        "for feat in features:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(train_desc[feat].dropna(), bins=30, alpha=0.6, label=\"Train\")\n",
        "    plt.hist(valid_desc[feat].dropna(), bins=30, alpha=0.6, label=\"Validation\")\n",
        "    plt.hist(test_desc[feat].dropna(),  bins=30, alpha=0.6, label=\"Test\")\n",
        "    plt.xlabel(feat)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.title(f\"Distribution of {feat}\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UOV3pO2y6yp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Model: Linear Regression\n",
        "\n",
        "We will start with the simplest possible machine-learning model — **Linear Regression**.\n",
        "\n",
        "It assumes that solubility can be expressed as a linear combination of molecular descriptors:\n",
        "\n",
        "$\\text{logS} = a_0 + a_1 \\cdot \\text{MolWt} + a_2 \\cdot \\text{logP} + a_3 \\cdot \\text{TPSA} + \\dots$\n",
        "\n",
        "\n",
        "Each descriptor contributes additively, and the model learns the coefficients that best fit the data.\n",
        "\n",
        "This model is easy to interpret — we can see which properties make a molecule **more** or **less** soluble.\n"
      ],
      "metadata": {
        "id": "Pab3NgFI8Mdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Define features and target\n",
        "features = [\"MolWt\", \"logP\", \"TPSA\", \"HBD\", \"HBA\", \"RotB\"]\n",
        "target = \"logS_exp\"\n",
        "\n",
        "X_train = train_desc[features]\n",
        "y_train = train_desc[target]\n",
        "X_test  = test_desc[features]\n",
        "y_test  = test_desc[target]\n",
        "\n",
        "# Train linear regression\n",
        "linreg = LinearRegression()\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = linreg.predict(X_test)\n",
        "\n",
        "print(\"R²:\", r2_score(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Intercept (constant):\", linreg.intercept_)\n",
        "\n",
        "# Show coefficients\n",
        "coef_df = pd.DataFrame({\n",
        "    \"Feature\": features,\n",
        "    \"Coefficient\": linreg.coef_\n",
        "}).sort_values(\"Coefficient\", ascending=False)\n",
        "\n",
        "coef_df\n"
      ],
      "metadata": {
        "id": "szWeXBnA8TFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(y_test, y_pred, alpha=0.7, edgecolor=\"k\", color=\"teal\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n",
        "plt.xlabel(\"Experimental log S\")\n",
        "plt.ylabel(\"Predicted log S\")\n",
        "plt.title(\"Linear Regression – Solubility Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_80D3t8c8WK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Regression\n",
        "\n",
        "Linear regression assumes a straight-line relationship between molecular descriptors and solubility.\n",
        "But chemistry is often **nonlinear**; small structural changes can have large, unpredictable effects on solubility.\n",
        "\n",
        "A **Random Forest Regressor** is an ensemble model that builds many small decision trees and averages their predictions.\n",
        "Each tree captures different relationships, allowing the forest to model more complex patterns.\n",
        "\n",
        "We will now train a Random Forest model on the same descriptors and compare its performance.\n"
      ],
      "metadata": {
        "id": "aIjBkta38-sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Same features as before (without logP for fair comparison)\n",
        "features_rf = [\"MolWt\", \"logP\", \"TPSA\", \"HBD\", \"HBA\", \"RotB\"]\n",
        "target = \"logS_exp\"\n",
        "\n",
        "X_train = train_desc[features_rf]\n",
        "y_train = train_desc[target]\n",
        "X_test  = test_desc[features_rf]\n",
        "y_test  = test_desc[target]\n",
        "\n",
        "# Train a Random Forest\n",
        "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"R²:\", r2_score(y_test, y_pred_rf))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "QIf6747_9A57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(y_test, y_pred_rf, alpha=0.7, edgecolor=\"k\", color=\"forestgreen\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\")\n",
        "plt.xlabel(\"Experimental log S\")\n",
        "plt.ylabel(\"Predicted log S\")\n",
        "plt.title(\"Random Forest – Solubility Prediction\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8OD7hASM9GD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing a Decision Tree from the Random Forest\n",
        "\n",
        "In a **Random Forest**, each individual decision tree learns simple *if–then* rules that split the data based on molecular descriptors (such as `MolWt`, `logP`, or `TPSA`).  \n",
        "Each split aims to **reduce the variance** in the target property (for example, solubility), separating molecules into more homogeneous groups.\n",
        "\n",
        "In the code below, we:\n",
        "- Select one trained tree from the forest (`rf.estimators_[0]`),\n",
        "- Plot only its **top few levels** (`max_depth=3`) to keep the figure readable,\n",
        "- Display each split’s **feature**, **threshold**, and **predicted mean value**.\n",
        "\n",
        "Each box in the plot represents a **node**:\n",
        "- **Decision nodes** show where the tree splits based on a descriptor threshold.  \n",
        "- **Leaf nodes** (the lowest boxes) store the **average predicted value** of all samples in that group.  \n",
        "- The **orange shading** indicates the predicted value — darker shades represent more negative solubility.\n",
        "\n",
        "Together, all trees in the Random Forest **average their predictions**, producing a smoother and more reliable model than any single tree alone.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Ni8yc00vn1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "# Pick one trained tree from the forest\n",
        "tree_idx = 0  # change to inspect another tree\n",
        "est = rf.estimators_[tree_idx]\n",
        "\n",
        "plt.figure(figsize=(18, 10))\n",
        "plot_tree(\n",
        "    est,\n",
        "    feature_names=features_rf,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    impurity=False,          # show fewer stats; set True if you want MSE\n",
        "    proportion=False,\n",
        "    max_depth=3,             # show only top levels so the plot stays readable\n",
        "    fontsize=10\n",
        ")\n",
        "plt.title(f\"RandomForestRegressor tree #{tree_idx} (top 3 levels)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YDD_z2xUncGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting with XGBoost\n",
        "\n",
        "A Random Forest builds many independent trees and averages their predictions.  \n",
        "In contrast, **gradient boosting** builds trees *sequentially*, where each new tree focuses on correcting the errors of the previous ones.  \n",
        "\n",
        "**XGBoost** (eXtreme Gradient Boosting) is a popular, efficient implementation of gradient boosting.  \n",
        "It often achieves better performance than Random Forests, especially when you have many correlated features or nonlinear relationships.\n",
        "\n",
        "We will train an `XGBRegressor` on our descriptors and see if it improves prediction of solubility.\n"
      ],
      "metadata": {
        "id": "5zHPzJAO9gYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q xgboost\n"
      ],
      "metadata": {
        "id": "di6m-O-B9gBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Select features (you could include logP or omit based on prior experiments)\n",
        "features = [\"MolWt\", \"logP\", \"TPSA\", \"HBD\", \"HBA\", \"RotB\"]\n",
        "target = \"logS_exp\"\n",
        "\n",
        "X_train = train_desc[features]\n",
        "y_train = train_desc[target]\n",
        "X_test  = test_desc[features]\n",
        "y_test  = test_desc[target]\n",
        "\n",
        "# Create and train XGBoost regressor\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    objective=\"reg:squarederror\",\n",
        "    random_state=42,\n",
        "    verbosity=0\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict & evaluate\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "print(\"XGBoost R²:\", r2_score(y_test, y_pred))\n",
        "print(\"XGBoost MAE:\", mean_absolute_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "r3ebjQcH9mgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chemprop – Learning Chemistry from Structure\n",
        "\n",
        "So far, we used *hand-crafted features* such as molecular weight, logP, and TPSA.\n",
        "These are intuitive but limited — they only capture part of the chemical information.\n",
        "\n",
        "**Chemprop** takes a different approach:\n",
        "it uses a **graph neural network (GNN)** that learns directly from the molecule’s graph representation.\n",
        "Atoms become nodes, bonds become edges, and the model learns patterns automatically.\n",
        "\n",
        "In other words:\n",
        "> Instead of telling the model what is important, we let it learn chemical features by itself.\n",
        "\n",
        "Chemprop is built on PyTorch and designed for property prediction from SMILES strings.\n",
        "It’s the same kind of technology used in modern drug-discovery ML pipelines.\n"
      ],
      "metadata": {
        "id": "A8RVWT_CDJ8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/chemprop/chemprop.git\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X_oIZFW5HN6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the CSV from GitHub\n",
        "!wget -q https://raw.githubusercontent.com/Svatunek-Lab/teaching/main/2025_University_of_Malaya_Intro_Chemistry_Python/data/delaney-processed.csv -O delaney.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"delaney.csv\")\n",
        "df = df.rename(columns={\"measured log solubility in mols per litre\": \"solubility_exp\"})\n",
        "df.to_csv(\"delaney.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "nIqLRIGPDnDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chemprop train \\\n",
        "    --data-path ./delaney.csv\\\n",
        "    --task-type regression \\\n",
        "    --smiles-columns smiles \\\n",
        "    --target-columns solubility_exp \\\n",
        "    --output-dir chemprop_model\n"
      ],
      "metadata": {
        "id": "Zl3qr9uCHSgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Load original data and Chemprop test predictions\n",
        "true = pd.read_csv(\"delaney.csv\")\n",
        "pred = pd.read_csv(\"chemprop_model/model_0/test_predictions.csv\")\n",
        "\n",
        "# Merge on SMILES\n",
        "merged = pd.merge(\n",
        "    pred,\n",
        "    true[['smiles', 'solubility_exp']],\n",
        "    on='smiles',\n",
        "    suffixes=('_pred', '_true')\n",
        ")[['smiles', 'solubility_exp_true', 'solubility_exp_pred']]\n",
        "\n",
        "# Save merged data\n",
        "merged.to_csv(\"chemprop_model/model_0/test_solubility_true_vs_pred.csv\", index=False)\n",
        "\n",
        "# Compute metrics\n",
        "r2 = r2_score(merged[\"solubility_exp_true\"], merged[\"solubility_exp_pred\"])\n",
        "mae = mean_absolute_error(merged[\"solubility_exp_true\"], merged[\"solubility_exp_pred\"])\n",
        "print(f\"R² = {r2:.3f}, MAE = {mae:.3f}\")\n",
        "\n",
        "# Plot experimental vs predicted\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(merged[\"solubility_exp_true\"], merged[\"solubility_exp_pred\"], alpha=0.7, edgecolor='k')\n",
        "plt.plot([-8, 2], [-8, 2], 'r--', label='Perfect correlation')\n",
        "plt.xlabel(\"Experimental solubility (logS)\")\n",
        "plt.ylabel(\"Predicted solubility (logS)\")\n",
        "plt.title(\"Chemprop Predictions on Test Set\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1dNK0igsOp1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chemprop predict \\\n",
        "  --model-path chemprop_model/model_0/best.pt \\\n",
        "  --smiles-columns smiles \\\n",
        "  --test-path delaney.csv \\\n",
        "  --preds-path chemprop_model/full_predictions_only.csv\n",
        "\n"
      ],
      "metadata": {
        "id": "_SZ7co4bHZEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Load original data and Chemprop full predictions\n",
        "true = pd.read_csv(\"delaney.csv\")\n",
        "pred = pd.read_csv(\"chemprop_model/full_predictions_only.csv\")\n",
        "\n",
        "# Merge on SMILES\n",
        "merged = pd.merge(\n",
        "    pred,\n",
        "    true[['smiles', 'solubility_exp']],\n",
        "    on='smiles',\n",
        "    suffixes=('_pred', '_true')\n",
        ")[['smiles', 'solubility_exp_true', 'solubility_exp_pred']]\n",
        "\n",
        "# Save merged data for reference\n",
        "merged.to_csv(\"chemprop_model/full_solubility_true_vs_pred.csv\", index=False)\n",
        "\n",
        "# Compute metrics\n",
        "r2 = r2_score(merged[\"solubility_exp_true\"], merged[\"solubility_exp_pred\"])\n",
        "mae = mean_absolute_error(merged[\"solubility_exp_true\"], merged[\"solubility_exp_pred\"])\n",
        "print(f\"R² = {r2:.3f}, MAE = {mae:.3f}\")\n",
        "\n",
        "# Plot experimental vs predicted\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(merged[\"solubility_exp_true\"], merged[\"solubility_exp_pred\"], alpha=0.7, edgecolor='k')\n",
        "plt.plot([-8, 2], [-8, 2], 'r--', label='Perfect correlation')\n",
        "plt.xlabel(\"Experimental solubility (logS)\")\n",
        "plt.ylabel(\"Predicted solubility (logS)\")\n",
        "plt.title(\"Chemprop Predictions (Full Dataset)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CXyxf_m9PRH5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}